{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "467c62fd",
   "metadata": {},
   "source": [
    "# Part 1 — Introduction to scraping data\n",
    "\n",
    "### What is web scraping?\n",
    "Web scraping is programmatically fetching content from websites and extracting structured data (tables, text, links, images, JSON, etc.). Scraping is useful for research, collecting datasets, monitoring prices, and more.\n",
    "\n",
    "### When to scrape vs. use an API\n",
    "\n",
    "Prefer an official API when available (cleaner, stable, respects provider limits).\n",
    "\n",
    "Scrape when there’s no API or the API lacks data you need — but proceed cautiously (see legal/ethical section).\n",
    "\n",
    "### High-level workflow\n",
    "\n",
    "- Inspect the page manually (browser dev tools).\n",
    "- Identify the URL(s) and the HTML elements or API endpoints that contain the data.\n",
    "- Write code to fetch the page (or call the API).\n",
    "- Parse the HTML/JSON and extract fields.\n",
    "- Respect robots.txt, rate limits, and terms of service; add delays and caching.\n",
    "- Store data (CSV, JSON, database)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5311a6",
   "metadata": {},
   "source": [
    "# Part 2 — Overview of popular scraping tools in Python\n",
    "\n",
    "Short intro to the toolset and when to choose each:\n",
    "\n",
    "1. requests + BeautifulSoup (bs4) — Classic stack for static HTML pages. Easy, lightweight, perfect for beginners and many real-world tasks where JS is not required.\n",
    "\n",
    "2. Scrapy — Full-featured scraping framework: spiders, request scheduling, pipelines, auto-throttling, built-in exporters, concurrent crawling. Use it for medium→large scale scraping projects.\n",
    "\n",
    "3. Selenium — Drives a real browser (or headless browser) and can handle dynamic pages, heavy JavaScript, and interactions (clicks, logins). Slower but powerful.\n",
    "\n",
    "4. APIs & JSON endpoints — Many sites load data via XHR/Fetch; reverse-engineering these endpoints is often easier and more reliable than parsing rendered HTML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b6dc76",
   "metadata": {},
   "source": [
    "# Part 3 — Simple use of BeautifulSoup\n",
    "\n",
    "```\n",
    "pip install bs4 requests\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478bbc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "\n",
    "# function to extract html document from given url\n",
    "def getHTMLdocument(url):\n",
    "    \n",
    "    # request for HTML document of given url\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # response will be provided in JSON format\n",
    "    return response.text\n",
    "\n",
    "  \n",
    "# assign required credentials\n",
    "# assign URL\n",
    "url_to_scrape = \"https://www.geeksforgeeks.org/courses\"\n",
    "\n",
    "# create document\n",
    "html_document = getHTMLdocument(url_to_scrape)\n",
    "\n",
    "# create soap object\n",
    "soup = BeautifulSoup(html_document, 'html.parser')\n",
    "\n",
    "\n",
    "# find all the anchor tags with \"href\" \n",
    "# attribute starting with \"https://\"\n",
    "for link in soup.find_all('a', attrs={'href': re.compile(\"^https://\")}):\n",
    "    # display the actual urls\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a48cf5",
   "metadata": {},
   "source": [
    "# Part 4 — Simple use of Scrapy\n",
    "\n",
    "Use Scrapy when you need concurrency, auto-throttling, pipelines for cleaning/exporting, retries, or a project structure for many pages.\n",
    "\n",
    "```\n",
    "pip install scrapy\n",
    "```\n",
    "```\n",
    "scrapy startproject my_scraper\n",
    "cd my_scraper\n",
    "scrapy crawl simple -o output.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905b6ba1",
   "metadata": {},
   "source": [
    "# Part 5 — Simple use of Selenium\n",
    "\n",
    "When the page content is rendered/assembled by JavaScript or needs interaction (logins, forms, clicking \"load more\").\n",
    "\n",
    "```\n",
    "pip install selenium webdriver-manager\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ef36bd",
   "metadata": {},
   "source": [
    "# Part 6 — Tips and traps\n",
    "\n",
    "- Anti-bot defenses\n",
    "\n",
    "    - Many sites use behavior analysis, JS challenges, CAPTCHAs, or rate-limit by IP. For legitimate large-scale crawling, contact the site owner for an API or data access.\n",
    "    - Do not use tools or techniques to bypass CAPTCHAs; it’s against terms of use and often illegal.\n",
    "\n",
    "- Common traps\n",
    "\n",
    "    - Dynamic content loaded after initial HTML (use browser automation or inspect network panel for JSON endpoints).\n",
    "    - Data hidden in embedded scripts — sometimes pages include JSON blobs inside script tags; parsing those is often easier and more reliable than HTML scraping.\n",
    "    - Pagination implemented by JavaScript — inspect XHR calls to find the real paginated endpoint.\n",
    "    - Locale / authentication changes content.\n",
    "\n",
    "- Debugging tips\n",
    "\n",
    "    - Start with requests + BeautifulSoup on a single page. Save the HTML to disk and debug selectors offline.\n",
    "    - Use curl -I to check headers and redirects.\n",
    "    - Use browser devtools (Network tab) to find JSON endpoints and to see exact request headers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
